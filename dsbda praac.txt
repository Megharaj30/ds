#A-1

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

iris=pd.read_csv("Iris.csv")
iris.head(10)


iris.isnull().sum()

iris.describe()

iris.shape

iris.dtypes


iris['Species']=iris['Species'].astype('category')

iris.dtypes


iris['Species_code']=iris['Species'].cat.codes

iris[['Species','Species_code']].head()


------------------------------------------------------------------------------------------------------------------------------------------


ass2:

import pandas as pd
import numpy as np

# Step 1: Create a sample dataset with missing values and inconsistencies
data = {
    'Student_ID': list(range(1, 51)),  # Creating Student IDs from 1 to 50
    'Name': ['John', 'Alice', 'Bob', 'Emily', 'Mike', 'Sophia', 'Liam', 'Olivia', 'Noah', 'Emma'] * 5,  # Repeating names to achieve 50 rows
    'Age': [20, 22, np.nan, 19, 25, 21, 23, 18, np.nan, 24] * 5,  # Repeating ages to achieve 50 rows
    'Gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'] * 5,  # Repeating genders to achieve 50 rows
    'Math_Score': [85, 90, 75, np.nan, 88, 92, 78, 85, 90, 85] * 5,  # Repeating math scores to achieve 50 rows
    'English_Score': [78, np.nan, 82, 80, 75, 85, 90, 88, 82, 86] * 5,  # Repeating English scores to achieve 50 rows
    'Physics_Score': [92, 85, 88, 94, np.nan, 85, 90, 84, 88, 92] * 5  # Repeating Physics scores to achieve 50 rows
}

df = pd.DataFrame(data)

# Check for missing values
print("Missing Values:")
print(df.isnull().sum())

# Step 2: Scan numeric variables for outliers
numeric_variables = ['Age', 'Math_Score', 'English_Score', 'Physics_Score']
outliers = {}
for var in numeric_variables:
    q1 = df[var].quantile(0.25)
    q3 = df[var].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers[var] = df[(df[var] < lower_bound) | (df[var] > upper_bound)][var].tolist()

print("\nOutliers:")
print(outliers)

# Step 3: Apply data transformations
# For example, let's apply a log transformation to the 'Math_Score' variable
df['Math_Score_Log'] = np.log(df['Math_Score'])

# Check skewness before and after transformation
print("\nSkewness Before Transformation:")
print(df['Math_Score'].skew())
print("\nSkewness After Transformation:")
print(df['Math_Score_Log'].skew())

# Display the updated DataFrame
print("\nUpdated DataFrame:")
print(df)



----------------------------------------------------------------------------------------------------------------------------


ass3:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


iris = sns.load_dataset('iris')

iris.head()


iris.describe()


iris.describe(include='object')


iris_groupby = iris.groupby(by='species')


iris_groupby.std()


iris_groupby.mean()

iris_groupby.median()

iris_groupby.min()

iris_groupby.max()


iris_groupby.quantile()
-----------------------------------------------------------------------------------------------------------------------------------


ass 4:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error


data=pd.read_csv("Boston.csv")
data.head()

X=data[['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','LSTAT']]
Y=data['MEDV']

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)


sc=StandardScaler()
X_test=sc.fit_transform(X_test)
X_train=sc.fit_transform(X_train)


lr=LinearRegression()
lr.fit(X_train,y_train)
y_pred=lr.predict(X_test)


print(y_pred)


rmse=np.sqrt(mean_squared_error(y_test,y_pred))
print("Root mean squared error is ",rmse)

print("train accuracy = ",lr.score(X_train,y_train))

print("test accuracy = ",lr.score(X_test,y_test))

--------------------------------------------------------------------------------------------------------------------------------------

ass5:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
data=pd.read_csv("Social_Network_Ads.csv")
data.head()


X=data[['Age','EstimatedSalary']]
Y=data['Purchased']

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.25)

sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.fit_transform(X_test)

lr=LogisticRegression()
lr.fit(X_train,y_train)
y_pred=lr.predict(X_test)

print(y_pred)

from sklearn.metrics import confusion_matrix,precision_score,accuracy_score,recall_score
cm=confusion_matrix(y_test,y_pred)
tn=cm[0][0]
fp=cm[0][1]
fn=cm[1][0]
tp=cm[1][1]

precision=precision_score(y_test,y_pred)
accuracy=accuracy_score(y_test,y_pred)
error=1-accuracy
recall=recall_score(y_test,y_pred)

print("confusion matrix \n",cm)
print("TN:",tn,"FN:",fn,"TP",tp,"FP",fp)
print("precision : %2f"%precision)
print("error : %2f"%error)
print("recall : %2f"%recall)
print("accuracy : %2f"%accuracy)


--------------------------------------------------------------------------------------------------------------------------------------


ass6:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix,recall_score,accuracy_score,precision_score

data=pd.read_csv("iris.csv")
data.head()

X=data[['sepal_length','sepal_width','petal_length','petal_width']]
Y=data['species']

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)

nb=GaussianNB()
nb.fit(X_train,y_train)
y_pred=nb.predict(X_test)

print(y_pred)

cm=confusion_matrix(y_test,y_pred)
tn=cm[0][0]
fp=cm[0][1]
fn=cm[1][0]
tp=cm[1][1]

precision=precision_score(y_test,y_pred,average='macro')
accuracy=accuracy_score(y_test,y_pred)
error=1-accuracy
recall=recall_score(y_test,y_pred,average='macro')

print("confusion matrix\n",cm)
print("TP",tp,"FP",fp,"TN",tn,"FN",fn,"\n")
print("precision = %2f"%precision)
print("accuracy = %2f"%accuracy)
print("recall = %2f"%recall)
print("error rate = %2f"%error)

---------------------------------------------------------------------------------------------------------------------------------------
ass7:    
#Tokenization: Split the text into words or tokens.
#POS Tagging: Tag each token with its part of speech.
#Stop words removal: Remove common stop words.
#Stemming: Reduce words to their root form using stemming.
#Lemmatization: Reduce words to their base or dictionary form using lemmatization.
    
# Install libraries if needed
# !pip install nltk scikit-learn

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')
nltk.download('wordnet')

# Sample document
doc = "Natural Language Processing is a fascinating field of Artificial Intelligence that focuses on the interaction between computers and humans through language."

# Preprocessing
tokens = doc.split()  # just simple split
tokens = [w for w in tokens if w.isalpha()]  # Keep only alphabet words
tokens = [w.lower() for w in tokens if w.lower() not in stopwords.words('english')]

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

stems = [stemmer.stem(w) for w in tokens]
lemmas = [lemmatizer.lemmatize(w) for w in tokens]

print("Tokens:", tokens)
print("Stems:", stems)
print("Lemmas:", lemmas)

# TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform([doc])

# Print TF-IDF
for word, score in zip(vectorizer.get_feature_names_out(), X.toarray()[0]):
    if score > 0:
        print(word, ":", round(score, 4))

-----------------------------------------------------------------------------------------------------------------------------------------


ass8:



import seaborn as sns
import matplotlib.pyplot as plt
from seaborn import load_dataset

# Load Titanic dataset from Seaborn
titanic_df = sns.load_dataset("titanic")

# Display the first few rows of the dataset to understand its structure
print(titanic_df.head())

# Use Seaborn to visualize patterns in the data
# For example, let's create a count plot of survival based on passenger class
sns.countplot(x='survived', hue='class', data=titanic_df)
plt.title('Survival Count by Passenger Class')
plt.show()

# Now, let's plot a histogram to visualize the distribution of ticket prices
sns.histplot(titanic_df['fare'], kde=True)
plt.title('Distribution of Ticket Prices')
plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.show()




-----------------------------------------------------------------------------------------------------------------------------------------



ass9:


import seaborn as sns
import matplotlib.pyplot as plt

# Load Titanic dataset from Seaborn
titanic_df = sns.load_dataset("titanic")

# Plotting box plot for distribution of age with respect to gender and survival status
sns.boxplot(x='sex', y='age', hue='survived', data=titanic_df)
plt.title('Distribution of Age by Gender and Survival Status')
plt.xlabel('Gender')
plt.ylabel('Age')
plt.show()


------------------------------------------------------------------------------------------------------------------------------------------



ass10:

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Download the Iris dataset from UCI Machine Learning Repository
iris_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

# Define column names for the dataset
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']

# Load the dataset into a DataFrame
iris_df = pd.read_csv(iris_url, names=column_names)

# 1. List down the features and their types
print("Features and their types:")
print(iris_df.dtypes)

# 2. Create a histogram for each feature
iris_df.hist(figsize=(10, 8))
plt.suptitle("Histograms of Iris Dataset Features")
plt.show()

# 3. Create a box plot for each feature
plt.figure(figsize=(10, 8))
sns.boxplot(data=iris_df)
plt.title("Boxplot of Iris Dataset Features")
plt.show()

# 4. Compare distributions and identify outliers
# We can visually inspect the histograms and box plots to compare distributions and identify outliers.

# Plot histograms and box plots side by side for comparison
fig, axs = plt.subplots(2, 4, figsize=(20, 10))

# Histograms
for i, col in enumerate(iris_df.columns[:-1]):
    sns.histplot(data=iris_df, x=col, ax=axs[0, i], kde=True)
    axs[0, i].set_title(f"Histogram of {col}")

# Box plots
for i, col in enumerate(iris_df.columns[:-1]):
    sns.boxplot(data=iris_df, y=col, ax=axs[1, i])
    axs[1, i].set_title(f"Boxplot of {col}")

plt.tight_layout()
plt.show()





-------------------------------------------------------------------------------------------------------------



